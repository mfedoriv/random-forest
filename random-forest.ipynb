{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest** или Случайный лес - модель, используемая как для регрессии, так и для классификации. Алгоритм основан на другом алгоритме - Decision Tree или Дерево решений, т.е. деревья используются для создания леса. Термин **\"случайный\"** говорит о том, что каждое дерево построено на основе случайной подвыборки данных.\n",
    "\n",
    "Для начала разберемся в том, как работает алгоритм Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Деревья решений состоят из двух элементов: узлы и ветви. Сам алгоритм основан на рекурсивном разбиении выборки по признаку, который даёт наибольший прирост информации (Gain), т.е. наилучшим образом разделяет выборку на данный момент. Разбиение будет происходить до тех пор, пока мы не дойдём до листьев - узлов, содержащих только экземпляры одного класса (pure, чистых). Пример дерева решений изображен на рисунке ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/decision_tree.svg\" width=70%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из рисунка, существует несколько типов узлов:\n",
    "\n",
    "1. **Корневой узел** &mdash; узел на вершине дерева. Содержит признак, который лучше всего делит данные (единственный признак, который классифицирует целевую переменную наиболее точно). Имеет 2 дочерних узла.\n",
    "2. **Узел** &mdash; внутренний узел дерева, узел проверки. Каждый такой узел имеет 2 дочерних узла.\n",
    "3. **Лист** &mdash; конечный узел дерева, узел решения. Не имеет потомков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки качества разбиения выборки по признаку используется **энтропия Шеннона**. Энтропия рассматривается как мера неоднородности подмножества по представленным в нем классам. Если классы представлены в равных долях, неопределенность классификации наибольшая, то и энтропия тоже максимальная. Логарифм от единицы будет обращать энтропию в ноль, если все примеры узла относятся к одному классу. Может быть расчитана по следующей формуле:\n",
    "$$H(T) = -\\sum_{i=1}^n p_i log_2 p_i = -\\sum_{i=1}^n \\frac{N_i}{N} log_2 \\left(\\frac{N_i}{N}\\right),$$\n",
    "где $T$ - вектор значений целевой переменной (target), $n$ — число классов в исходном подмножестве, $N_i$ — число экземпляров i-го класса в подмножестве, $N$ — общее число экземпляров в подмножестве."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(t):\n",
    "    counts = np.bincount(t)\n",
    "    probability = counts / len(t)\n",
    "    \n",
    "    entropy = 0\n",
    "    for prob in probability:\n",
    "        if prob > 0:\n",
    "            entropy += prob * np.log2(prob)\n",
    "    return -entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 0.88129\n"
     ]
    }
   ],
   "source": [
    "s = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
    "print(f'Entropy: {np.round(entropy(s), 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На практике, однако, говорят не об энтропии, а о величине, обратной ей, которая называется **Приростом информации (Inforamtion Gain, IG)**. Тогда лучшим атрибутом разбиения будет тот, который обеспечит максимальный прирост информации результирующего узла относительно исходного. \n",
    "Прирост информации представляет собой разницу между энтропией исходной выборки и взвешенным средним суммы всех значений энтропии для разделённых подмножеств. Чем выше значение информационного прироста, тем лучше принятое решение. Может быть расчитан по следующей формуле:\n",
    "$$IG(T, A) = H(T) - H(T|A)$$\n",
    "$$IG(T, A) = H(T) - \\sum_{v \\in Values(A)}\\frac{|T_{v}|}{|T|}H(T_{v}),$$\n",
    "где $T$ - вектор значений целевой переменной (target), $A$ - признак, по которому было совершено разбиение, $v$ - каждое уникальное значение признака $A$.\n",
    "\n",
    "Если объяснить немного по-другому, мы находим энтропию каждого набора после разбиения, взвешиваем ее по количеству элементов в каждом разбиении, затем вычитаем из текущей энтропии. Если результат положительный, значит, мы снизили энтропию при разбиении. Чем выше результат, тем больше мы понизили энтропию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(parent, left, right):\n",
    "    num_left = len(left) / len(parent)\n",
    "    num_right = len(right) / len(parent)\n",
    "    \n",
    "    gain = entropy(parent) - (num_left * entropy(left) + num_right * entropy(right))\n",
    "    return gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information gain: 0.18094\n"
     ]
    }
   ],
   "source": [
    "parent = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "left_child = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
    "right_child = [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "print(f'Information gain: {np.round(information_gain(parent, left_child, right_child), 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация Decision Tree на Python\n",
    "Для реализации необходимы 2 класса:\n",
    "* Node - реализует один узел дерева решений\n",
    "* DecisionTree - реализует алгоритм дерева решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Класс, реализующий один узел дерева\n",
    "Класс, необходимый для хранения данных о признаке(feature) и пороговом значении(threshold), по которым производилось разбиение выборки, поддеревья левое(data_left) и правое(data_right), информационном приросте(gain) и значении классификации(value), которое устанавливается только для листовых узлов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, data_left=None, data_right=None, gain=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.data_left = data_left\n",
    "        self.data_right = data_right\n",
    "        self.gain = gain\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Класс, реализующий дерево решений\n",
    "Класс содержит следующие методы:\n",
    "1. Конструктор **\\_\\_init__**, в котором задаются значения для min_samples_split и max_depth. Это гиперпараметры. Первый используется для указания минимального количества образцов, необходимых для разбиения узла, а второй задает максимальную глубину дерева. Оба параметра используются в рекурсивных функциях в качестве условий выхода.\n",
    "1. Метод **\\_entropy(t)** вычисляет энтропию входного вектора t\n",
    "1. Метод **\\_information_gain(parent, left, right)** вычисляет значение информационного прироста при разделении родительского и двух дочерних векторов.\n",
    "1. Метод **\\_best_split(X, y)** вычисляет наилучшие параметры разбиения для входных признаков X и целевой переменной y. Это делается путем итерации по каждому столбцу в X и по каждому пороговому значению в каждом столбце, чтобы найти оптимальное разбиение с использованием информационного прироста.\n",
    "1. Метод **\\_build(X, y, depth)** рекурсивно строит дерево решений до достижения критериев останова (гиперпараметры в конструкторе).\n",
    "1. Метод **fit(X, y)** вызывает функцию **\\_build()** и сохраняет построенное дерево\n",
    "1. Метод **\\_predict(x)** обходит дерево для классификации одного экземпляра\n",
    "1. Метод **predict(X)** применяет функцию **\\_predict()** к каждому экземпляру в матрице X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    '''\n",
    "    Класс, реализующий классификатор на основе алгоритма дерева решений\n",
    "    '''\n",
    "    def __init__(self, min_samples_split=2, max_depth=5):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def _entropy(t):\n",
    "        '''\n",
    "        Вспомогательная функция, вычисляет энтропию массива целочисленных значений\n",
    "        '''\n",
    "        counts = np.bincount(np.array(t, dtype=np.int))\n",
    "        probability = counts / len(t)\n",
    "    \n",
    "        entropy = 0\n",
    "        for prob in probability:\n",
    "            if prob > 0:\n",
    "                entropy += prob * np.log2(prob)\n",
    "        return -entropy\n",
    "    \n",
    "    def _information_gain(self, parent, left, right):\n",
    "        '''\n",
    "        Вспомогательная функция, вычисляет информационный прирост от родителя и двух узлов-потомков\n",
    "        '''\n",
    "        num_left = len(left) / len(parent)\n",
    "        num_right = len(right) / len(parent)\n",
    "    \n",
    "        gain = self._entropy(parent) - (num_left * self._entropy(left) + num_right * self._entropy(right))\n",
    "        return gain\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        '''\n",
    "        Вспомогательная функция, вычисляет лучшее разбиение для заданных признаков и меток\n",
    "        '''\n",
    "        best_split = {}\n",
    "        best_info_gain = -1\n",
    "        n_rows, n_cols = X.shape\n",
    "        \n",
    "        # for every feature\n",
    "        for f_idx in range(n_cols):\n",
    "            X_curr = X[:, f_idx]\n",
    "            # for every unuque value of feature\n",
    "            for threshold in np.unique(X_curr):\n",
    "                df = np.concatenate((X, y.reshape(1, -1).T), axis=1)\n",
    "                df_left = np.array([row for row in df if row[f_idx] <= threshold])\n",
    "                df_right = np.array([row for row in df if row[f_idx] > threshold])\n",
    "            \n",
    "                if len(df_left) > 0 and len(df_right) > 0:\n",
    "                    y = df[:, -1]\n",
    "                    y_left = df_left[:, -1]\n",
    "                    y_right = df_right[:, -1]\n",
    "                \n",
    "                    gain = self._information_gain(y, y_left, y_right)\n",
    "                    if gain > best_info_gain:\n",
    "                        best_split = {\n",
    "                            'feature_index': f_idx,\n",
    "                            'threshold': threshold,\n",
    "                            'df_left': df_left,\n",
    "                            'df_right': df_right,\n",
    "                            'gain': gain\n",
    "                        }\n",
    "                        best_info_gain = gain\n",
    "        return best_split\n",
    "    \n",
    "    def _build(self, X, y, depth=0):\n",
    "        '''\n",
    "        Вспомогательная рекурсивная функция, используется для построения дерева решений по входным данным\n",
    "        '''\n",
    "        n_rows, n_cols = X.shape\n",
    "        \n",
    "        if n_rows >= self.min_samples_split and depth <= self.max_depth:\n",
    "            best = self._best_split(X, y)\n",
    "            if best['gain'] > 0: # impure split\n",
    "                left = self._build(\n",
    "                    X=best['df_left'][:, :-1], \n",
    "                    y=best['df_left'][:, -1], \n",
    "                    depth=depth + 1\n",
    "                )\n",
    "                right = self._build(\n",
    "                    X=best['df_right'][:, :-1], \n",
    "                    y=best['df_right'][:, -1], \n",
    "                    depth=depth + 1\n",
    "                )\n",
    "                return Node(\n",
    "                    feature=best['feature_index'], \n",
    "                    threshold=best['threshold'], \n",
    "                    data_left=left, \n",
    "                    data_right=right, \n",
    "                    gain=best['gain']\n",
    "                )\n",
    "        # лист (pure), value - самое часто встречающееся значение в y\n",
    "        return Node(\n",
    "            value=int(Counter(y).most_common(1)[0][0])\n",
    "        )\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Функция для обучения классификатора дерева решений\n",
    "        '''\n",
    "        self.root = self._build(X, y)\n",
    "    \n",
    "    def _predict(self, x, tree):\n",
    "        '''\n",
    "        Вспомогательная рекурсивная функция для предсказания одного значения (обход дерева)\n",
    "        '''\n",
    "        # leaf\n",
    "        if tree.value != None:\n",
    "            return tree.value\n",
    "        feature_value = x[tree.feature]\n",
    "        \n",
    "        # go left\n",
    "        if feature_value <= tree.threshold:\n",
    "            return self._predict(x=x, tree=tree.data_left)\n",
    "        \n",
    "        # go right\n",
    "        if feature_value > tree.threshold:\n",
    "            return self._predict(x=x, tree=tree.data_right)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Функция для предсказания значений меток\n",
    "        '''\n",
    "        return np.array([self._predict(x, self.root) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris['data']\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним построенную модель с классификатором из библиотеки Scikit-Learn DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "sk_model = DecisionTreeClassifier()\n",
    "sk_model.fit(X_train, y_train)\n",
    "sk_y_pred = sk_model.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, sk_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Алгоритм Случайного леса состоит из трёх компонентов:\n",
    "$$Random Forest = Decision Tree + Bagging + Random Subspace$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/comparison.png\" width=70%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация Random Forest на Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс содержит следующие методы:\n",
    "\n",
    "1. **\\_\\_init__()** - конструктор, хранит значения гиперпараметров для количества деревьев в лесе, минимального количества элементовв разбиении дерева и максимальной глубины. В нём также будут храниться индивидуально обученные деревья решений после обучения модели.\n",
    "1. **\\_sample(X, y)** применяет bootstrap-выборку к входным признакам и входной целевой переменной\n",
    "1. **fit(X, y)** метод для обучения модели классификатора\n",
    "1. **predict(X)** делает прогнозы с помощью отдельных деревьев решений, а затем применяет мажоритарное голосование для окончательного прогноза"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    '''\n",
    "    Класс, реализующий алгоритм Слечайного леса\n",
    "    '''\n",
    "    def __init__(self, num_trees=25, min_samples_split=2, max_depth=5):\n",
    "        self.num_trees = num_trees\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        # Храним все построенные деревья\n",
    "        self.decision_trees = []\n",
    "\n",
    "    @staticmethod\n",
    "    def _sample(X, y):\n",
    "        '''\n",
    "        Вспомогательная функция для создания bootstrap-выборки\n",
    "        '''\n",
    "        n_rows, n_cols = X.shape\n",
    "        # Sample with replacement\n",
    "        samples = np.random.choice(a=n_rows, size=n_rows, replace=True)\n",
    "        return X[samples], y[samples]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Обучает классификатор Случайный лес\n",
    "        '''\n",
    "        # Очистка списка деревьев при повторном запуске обучения\n",
    "        if len(self.decision_trees) > 0:\n",
    "            self.decision_trees = []\n",
    "            \n",
    "        # Построение каждого дерева леса\n",
    "        num_built = 0\n",
    "        while num_built < self.num_trees:\n",
    "            try:\n",
    "                clf = DecisionTree(\n",
    "                    min_samples_split=self.min_samples_split,\n",
    "                    max_depth=self.max_depth\n",
    "                )\n",
    "                # Получаем подвыборку\n",
    "                _X, _y = self._sample(X, y)\n",
    "                # Обучаем\n",
    "                clf.fit(_X, _y)\n",
    "                # Сохраняем классификатор\n",
    "                self.decision_trees.append(clf)\n",
    "                num_built += 1\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Предсказывает метку класса для новых объектов\n",
    "        '''\n",
    "        # Сделать предсказание каждым деревом в лесу\n",
    "        y = []\n",
    "        for tree in self.decision_trees:\n",
    "            y.append(tree.predict(X))\n",
    "        \n",
    "        # Решейп для поиска наиболее частого значения\n",
    "        y = np.swapaxes(a=y, axis1=0, axis2=1)\n",
    "        \n",
    "        # Используем голосование большинством голосов для финального предсказания\n",
    "        predictions = []\n",
    "        for preds in y:\n",
    "            counter = Counter(x)\n",
    "            predictions.append(counter.most_common(1)[0][0])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
